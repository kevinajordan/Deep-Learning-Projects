{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generating_headlines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/Deep-Learning-Projects/blob/master/generating-headlines/generating_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURE6kiy8czR",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with Attention - Generating News Headlines\n",
        "\n",
        "This notebook attempts to implement the model described in this research paper:\n",
        "\n",
        "[Generating News Headlines with Recurrent Neural\n",
        "Networks](https://arxiv.org/pdf/1512.01712.pdf)\n",
        "\n",
        "## Bahdanau Attention\n",
        "\n",
        "For getting an intuitive understanding of attention and how I implemented it in this notebook, check this blog post out: [Attention in Deep Networks with Keras](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)\n",
        "\n",
        "![Bahdanau Attention](https://miro.medium.com/max/1638/1*wcxAAgQ0n9gOXLRqhmaLGA.png)\n",
        "\n",
        "### News Dataset\n",
        "The English Gigaword dataset mentioned in the research paper costs $3,000 (https://catalog.ldc.upenn.edu/LDC2003T05). \n",
        "\n",
        "Since I don't want to spend that in order to do this project, I am substituting in this kaggle news dataset:\n",
        "\n",
        "* https://www.kaggle.com/snapcrack/all-the-news/data\n",
        "\n",
        "\n",
        "Due to the kaggle news dataset being much cleaner than the English Gigawords dataset, the data preprocessing steps will not be the same as mentioned in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4W66vyo7Ly7",
        "colab_type": "code",
        "outputId": "e77b8556-3a03-40e8-fd77-b239a456f583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/kevinajordan/Deep-Learning-Projects.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep-Learning-Projects'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Total 54 (delta 0), reused 0 (delta 0), pack-reused 54\n",
            "Unpacking objects: 100% (54/54), done.\n",
            "Checking out files: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNAnvvAOCTHv",
        "colab_type": "code",
        "outputId": "df2bd1c9-c766-4d5f-c922-cff1077a99ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "!ls -la Deep-Learning-Projects/generating-headlines/data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1307920\n",
            "drwxr-xr-x 3 root root      4096 Oct  7 21:09 .\n",
            "drwxr-xr-x 3 root root      4096 Oct  7 21:03 ..\n",
            "drwxrwxrwx 2 root root      4096 Oct  7 21:10 all-the-news\n",
            "-rw-r--r-- 1 root root 669644288 Oct  7 21:07 all-the-news.tar\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.001\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.002\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.003\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.004\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.005\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.006\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.007\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.008\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.009\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.010\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.011\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.012\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.013\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.014\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.015\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.016\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.017\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.018\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.019\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.020\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.021\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.022\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.023\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.024\n",
            "-rw-r--r-- 1 root root  26214400 Oct  7 21:03 all-the-news.tar.025\n",
            "-rw-r--r-- 1 root root  14284288 Oct  7 21:03 all-the-news.tar.026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2YoN7smI-1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat Deep-Learning-Projects/generating-headlines/data/all-the-news.tar.0* > all-the-news.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBzfBoVJJDu0",
        "colab_type": "code",
        "outputId": "61751c1d-0547-48af-881b-3b8955bb2277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!tar -xvf all-the-news.tar "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all-the-news/\n",
            "all-the-news/articles1.csv\n",
            "all-the-news/articles2.csv\n",
            "all-the-news/articles3.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUCyWyvFCoW3",
        "colab_type": "code",
        "outputId": "4ad216a6-1401-4f86-93ff-1aea8a19d68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os \n",
        "os.chdir('all-the-news')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/all-the-news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhDCYSW0DyXk",
        "colab_type": "code",
        "outputId": "1ab93d05-f702-4559-9770-c809a3aace73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls -la /content/all-the-news/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 653960\n",
            "drwxrwxrwx 2 root root      4096 Oct  2 14:20 .\n",
            "drwxr-xr-x 1 root root      4096 Oct  7 21:34 ..\n",
            "-rwxrwxrwx 1 root root 203539364 Sep 21 03:26 articles1.csv\n",
            "-rwxrwxrwx 1 root root 225757056 Sep 21 03:26 articles2.csv\n",
            "-rwxrwxrwx 1 root root 240344348 Sep 21 03:27 articles3.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzsFVpNHZvbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd, re, itertools, collections, nltk, string, random, unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMeJoCIoFoO",
        "colab_type": "code",
        "outputId": "302b631d-46f3-4809-d7eb-5e32dbe81faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr1RsK-2jx4g",
        "colab_type": "code",
        "outputId": "9b59ed6f-fa54-47e3-9c82-6202bcaebda9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import glob\n",
        "_files = glob.glob(\"*.csv\")\n",
        "print(_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['articles3.csv', 'articles2.csv', 'articles1.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-vbbFM1hpky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_list = []\n",
        "for filename in sorted(_files):\n",
        "    df_list.append(pd.read_csv(filename))\n",
        "full_df = pd.concat(df_list)\n",
        "full_df.to_csv('articles.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5LdZ46TkcHC",
        "colab_type": "code",
        "outputId": "7bd102ff-5921-4393-d4f5-be17c9607d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "full_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     id  ...  url                                            content\n",
              "0           0  17283  ...  NaN  WASHINGTON  —   Congressional Republicans have...\n",
              "1           1  17284  ...  NaN  After the bullet shells get counted, the blood...\n",
              "2           2  17285  ...  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3           3  17286  ...  NaN  Death may be the great equalizer, but it isn’t...\n",
              "4           4  17287  ...  NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKXpFqfplbb4",
        "colab_type": "code",
        "outputId": "e46d4139-ec18-4b8b-a33d-673b47e068c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "full_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0         0\n",
              "id                 0\n",
              "title              2\n",
              "publication        0\n",
              "author         15876\n",
              "date            2641\n",
              "year            2641\n",
              "month           2641\n",
              "url            57011\n",
              "content            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZc0iEQxNltS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_df.drop(columns=['author','url'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bL4H5fznY9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(full_df))\n",
        "full_df.dropna(axis = 0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbupzAgdnwmY",
        "colab_type": "code",
        "outputId": "6400e438-b6c4-4001-b668-cd78e47aa401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(full_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139927"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_PIwDM0DxLC",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "* Headline and text are converted to lowercase\n",
        "* Punctuation is separated from words.\n",
        "* Headline and text are tokenized. \n",
        "* end-of-sequence token is added to both the headline and text.\n",
        "* Articles with no headline or text are removed.\n",
        "* All rare words are replaced with <unk> symbols, with only the 40k most frequent words kept.\n",
        "\n",
        "### Dataset Forming\n",
        "The data is split into a training and a holdout set. The holdout set consists of articles from the last month of data, with the second last month not included in either the training or holdout sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aamf5qt8_IJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converts all characters to lowercase\n",
        "full_df.title = full_df.title.str.lower()\n",
        "full_df.content = full_df.content.str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSe9EclR3ANk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting all of the values for the headlines and content. data type stored are numpy arrays\n",
        "headlines = full_df.title.values\n",
        "content = full_df.content.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQF2wTvXDb6O",
        "colab_type": "code",
        "outputId": "18ac3785-a642-46f7-dae4-126efcd1e6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "content[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'WASHINGTON — Congressional Republicans have a new fear when it comes to their health care lawsuit against the Obama administration: They might win. The incoming Trump administration could choose to no longer defend the executive branch against the suit, which challenges the administration’s authority to spend billions of dollars on <eos>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9ODJAdy5zuV",
        "colab_type": "code",
        "outputId": "592faada-1110-4ab6-e322-a5f6c4503a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "headlines[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'House Republicans Fret About Winning Their Health Care Suit - The New York Times'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFFcZHAU1k-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(content)):\n",
        "    #limit the content of each article to the first 50 words\n",
        "    desc = content[i].split()\n",
        "    desc = ' '.join(desc[:50])\n",
        "    # appending <eos> tags to each article content\n",
        "    content[i] = desc + ' <eos>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYFjwtglaK7f",
        "colab_type": "code",
        "outputId": "7d2e2765-16f3-490a-f901-f18f2590aad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# confirm it works\n",
        "print(content[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WASHINGTON — Congressional Republicans have a new fear when it comes to their health care lawsuit against the Obama administration: They might win. The incoming Trump administration could choose to no longer defend the executive branch against the suit, which challenges the administration’s authority to spend billions of dollars on <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhwFn7tnom1",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uls1LD2g06ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose the most frequent 5000 words from the vocabulary\n",
        "import tensorflow as tf\n",
        "words_limit = 40000\n",
        "# filter acts the same as removing punctuation\n",
        "# oov_token replaces every word past num_words with specified tag/token\n",
        "# 'lower' parameter converts the text to lowercase\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=words_limit,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ', lower=True)\n",
        "# trains on the text\n",
        "tokenizer.fit_on_texts(content)\n",
        "# apply tokenizer to content. Converts all words to integers\n",
        "content = tokenizer.texts_to_sequences(content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcr1ElLdD9RE",
        "colab_type": "code",
        "outputId": "9837d9d3-2f29-4b34-eb70-52af54ba1a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "content[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[95,\n",
              " 18,\n",
              " 708,\n",
              " 245,\n",
              " 29,\n",
              " 3,\n",
              " 32,\n",
              " 1523,\n",
              " 47,\n",
              " 25,\n",
              " 412,\n",
              " 5,\n",
              " 44,\n",
              " 211,\n",
              " 350,\n",
              " 775,\n",
              " 108,\n",
              " 2,\n",
              " 113,\n",
              " 177,\n",
              " 48,\n",
              " 287,\n",
              " 405,\n",
              " 2,\n",
              " 2238,\n",
              " 22,\n",
              " 177,\n",
              " 112,\n",
              " 2255,\n",
              " 5,\n",
              " 107,\n",
              " 963,\n",
              " 2440,\n",
              " 2,\n",
              " 241,\n",
              " 4877,\n",
              " 108,\n",
              " 2,\n",
              " 2155,\n",
              " 64,\n",
              " 2337,\n",
              " 2,\n",
              " 1359,\n",
              " 2049,\n",
              " 5,\n",
              " 1891,\n",
              " 3419,\n",
              " 4,\n",
              " 1389,\n",
              " 9,\n",
              " 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKtfaZedqMXU",
        "colab_type": "text"
      },
      "source": [
        "## WIP: Using Pre-Trained Word Embeddings - Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWV85ievdKEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U45KY6Fqa9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading word vectors\n",
        "import gensim\n",
        "from gensim import models\n",
        "# Download Word Vectors\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R8LrmbVqH43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_word_embedding(file_name):\n",
        "    \"\"\"\n",
        "    read word embedding file and assign indexes to word\n",
        "    \"\"\"\n",
        "    idx = 3\n",
        "    temp_word2vec_dict = {}\n",
        "    # <empty>, <eos> tag replaced by word2vec learning\n",
        "    # create random dimensional vector for empty, eos and unk tokens\n",
        "    temp_word2vec_dict['<empty>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n",
        "    temp_word2vec_dict['<eos>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n",
        "    temp_word2vec_dict['<unk>'] = [float(i) for i in np.random.rand(embedding_dimension, 1)]\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(file_name, binary = True, limit = 40000)\n",
        "    V = model.index2word\n",
        "    X = np.zeros((top_freq_word_to_use, model.vector_size))\n",
        "    for index, word in enumerate(V):\n",
        "        vector = model[word]\n",
        "        temp_word2vec_dict[idx] = vector\n",
        "        word2idx[word] = idx\n",
        "        idx2word[idx] = word\n",
        "        idx = idx + 1\n",
        "        if idx % 10000 == 0:\n",
        "            print (\"working on word2vec ... idx \", idx)\n",
        "            \n",
        "    return temp_word2vec_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VePzYP1NqL4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_word2vec_dict = read_word_embedding('GoogleNews-vectors-negative300.bin.gz')\n",
        "length_vocab = len(temp_word2vec_dict)\n",
        "shape = (length_vocab, embedding_dimension)\n",
        "# faster initlization and random for <empty> and <eos> tag\n",
        "word2vec = np.random.uniform(low=-1, high=1, size=shape)\n",
        "for i in range(length_vocab):\n",
        "    if i in temp_word2vec_dict:\n",
        "        word2vec[i, :] = temp_word2vec_dict[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxEsbdKIoOh3",
        "colab_type": "code",
        "outputId": "4b2ae5fd-d7e1-4ed1-a9f7-a884660af6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(dictionary)\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 4778956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihmJOF34oSxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk_len = 10000\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti_WczXEtDQ1",
        "colab_type": "text"
      },
      "source": [
        "## Create LSTMs for Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQMhpuBGVDpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=384, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  lstm_3 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_2)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_3)\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DiDxBvsoiK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.Input(shape)\n",
        "# Encoding layers\n",
        "embedding = tf.keras.layers.Embedding()\n",
        "lstm1 = tf.keras.layers.LSTM(return_sequences=True)(embedding)\n",
        "lstm2 = tf.keras.layers.LSTM()(lstm1)\n",
        "attention1 = tf.keras.layers.Attention()(lstm2)\n",
        "\n",
        "#Decoding layers\n",
        "tf.keras.layers.Embedding()\n",
        "tf.keras.layers.LSTM()\n",
        "tf.keras.layers.LSTM()\n",
        "tf.keras.layers.Attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKD4CrLNOG00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, codecs, math, time, sys, subprocess, os.path, pickle\n",
        "import numpy as np, pandas as pd \n",
        "import gensim\n",
        "from numpy import inf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvFk2uCvqVYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(dictionary)\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r8VAKpuqYLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk_len = 10000\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}