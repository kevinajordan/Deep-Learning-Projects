{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generating_headlines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/Deep-Learning-Projects/blob/master/generating-headlines/generating_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURE6kiy8czR",
        "colab_type": "text"
      },
      "source": [
        "# LSTM with Attention - Generating News Headlines\n",
        "\n",
        "This notebook attempts to implement the model described in this research paper:\n",
        "\n",
        "[Generating News Headlines with Recurrent Neural\n",
        "Networks](https://arxiv.org/pdf/1512.01712.pdf)\n",
        "\n",
        "### News Dataset\n",
        "The English Gigaword dataset mentioned in the research paper costs $3,000 (https://catalog.ldc.upenn.edu/LDC2003T05). \n",
        "\n",
        "Since I don't want to spend that in order to do this project, I am substituting in this kaggle news dataset mentioned below:\n",
        "\n",
        "* https://www.kaggle.com/snapcrack/all-the-news/data\n",
        "\n",
        "Due to the kaggle news dataset being much cleaner than the English Gigawords dataset, the data preprocessing steps will not be the same as mentioned in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIji5Vu4EuTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading word vectors\n",
        "import gensim\n",
        "from gensim import models\n",
        "# Download Word Vectors\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vfLtiCOAy1",
        "colab_type": "code",
        "outputId": "6eab945e-541e-4304-fcd6-1d4d96af4bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzsFVpNHZvbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np, pandas as pd, re, itertools, collections, nltk, string, random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOyWOBsVZ-bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ebf2f5c8-145a-4d45-85c8-baf2586e5da5"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdxbAM_Y2Vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6fbaf769-7fd0-48d1-e9a8-f48b3e0be1cc"
      },
      "source": [
        "\n",
        "df=pd.read_csv(\"articles1.csv\")\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     id  ... url                                            content\n",
              "0           0  17283  ... NaN  WASHINGTON  —   Congressional Republicans have...\n",
              "1           1  17284  ... NaN  After the bullet shells get counted, the blood...\n",
              "2           2  17285  ... NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3           3  17286  ... NaN  Death may be the great equalizer, but it isn’t...\n",
              "4           4  17287  ... NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM-6DFIdPZDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"articles1.csv\")\n",
        "df = df.iloc[:1000,:]\n",
        "dictionary = list(df.content)\n",
        "def joinStrings(dictionary):\n",
        "    return ' '.join(string for string in dictionary)\n",
        "\n",
        "dictionary = joinStrings(dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyAZZcEDqSIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = set(nltk.corpus.stopwords.words('english'))\n",
        "exclude = set(string.punctuation) \n",
        "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "        stop_free = \" \".join([i for i in doc.split() if i not in stop])\n",
        "        punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "        return normalized\n",
        "dictionary = clean(dictionary).lower()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKD4CrLNOG00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, codecs, math, time, sys, subprocess, os.path, pickle\n",
        "import numpy as np, pandas as pd \n",
        "import gensim\n",
        "from numpy import inf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx3khOFsO44W",
        "colab_type": "code",
        "outputId": "95b7b042-3c0f-44b4-c7ac-63cb14b90081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "'''\n",
        "from tf.keras.models import Sequential\n",
        "from tf.keras.layers.core import Dense\n",
        "from tf.keras.layers.wrappers import TimeDistributed\n",
        "from tf.keras.layers.recurrent import LSTM\n",
        "from tf.keras.layers.embeddings import Embedding\n",
        "from tf.keras.layers.core import Lambda, Activation\n",
        "from tf.keras.utils import np_utils\n",
        "from tf.keras.preprocessing import sequence\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e306fb291a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvFk2uCvqVYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(dictionary)\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r8VAKpuqYLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk_len = 10000\n",
        "\n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}