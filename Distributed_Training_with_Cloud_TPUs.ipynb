{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Distributed Training with Cloud TPUs and Keras",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/Deep-Learning-Projects/blob/master/Distributed_Training_with_Cloud_TPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "edfbxDDh2AEs"
      },
      "source": [
        "## Predict Shakespeare with Cloud TPUs and Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KRQ6Fjra3Ruq"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j8sIXh1DEDDd",
        "outputId": "3cdd8431-8ddd-4d91-89f7-e5062bc07c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-10 21:38:46--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5784758 (5.5M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.52M  4.65MB/s    in 1.2s    \n",
            "\n",
            "2019-10-10 21:38:47 (4.65 MB/s) - ‘/content/shakespeare.txt’ saved [5784758/5784758]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AbL6cqCl7hnt"
      },
      "source": [
        "### Build the input dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7nbGKAHi0dx",
        "colab_type": "text"
      },
      "source": [
        "We just downloaded some text. The following shows the start of the text and a random snippet so we can get a feel for the whole text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3V4V-Jxmuv3",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# This address identifies the TPU to use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "def transform(txt):\n",
        "  # character tokenizing\n",
        "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "  with tf.io.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
        "\n",
        "  def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "  BUFFER_SIZE = 10000\n",
        "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "  return ds.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS3mphL35P53",
        "colab_type": "code",
        "outputId": "e17cb4fd-1ad8-4bb0-8611-0ba6106f5636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(SHAKESPEARE_TXT).read()\n",
        "print(len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5584464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAlQjcANhyjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.asarray([ord(c) for c in text if ord(c) < 255], dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiQw_LjJichR",
        "colab_type": "code",
        "outputId": "1a0fca2a-30f1-42c6-e324-47229f70b4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 10,  80, 114, ...,  10,  10,  10], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bbb05dNynDrQ"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The model is defined as a three-layer, forward-LSTM.\n",
        "\n",
        "Because our vocabulary size is 256, the input dimension to the Embedding layer is 256.\n",
        "\n",
        "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLEM-fLJlEEt",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VzBYDJI0_Tfm"
      },
      "source": [
        "### Train the model - distribute across multiple TPU's\n",
        "\n",
        "Very simple 1-layer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExQ922tfzSGA",
        "outputId": "04ab7fa9-15ae-4527-ed2b-dc18974c5c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  training_model = lstm_model(seq_len=100, stateful=False)\n",
        "  training_model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=50\n",
        ")\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.71.132.250:8470\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.71.132.250:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 840594599913837694)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15697445970489885833)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12015926544250007747)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7580791680063379848)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3479545019033869365)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3736126347286450847)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9371252790609291912)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13669502364015188836)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10989323045075752567)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17723830885264604129)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15381558760710390801)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/50\n",
            "100/100 [==============================] - 14s 137ms/step - loss: 3.3925 - sparse_categorical_accuracy: 0.1799\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 11s 107ms/step - loss: 2.9300 - sparse_categorical_accuracy: 0.2153\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 11s 108ms/step - loss: 2.3968 - sparse_categorical_accuracy: 0.3338\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 11s 108ms/step - loss: 2.1842 - sparse_categorical_accuracy: 0.3842\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 11s 108ms/step - loss: 2.0257 - sparse_categorical_accuracy: 0.4234\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 1.8983 - sparse_categorical_accuracy: 0.4622\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 11s 110ms/step - loss: 1.7910 - sparse_categorical_accuracy: 0.4913\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 11s 110ms/step - loss: 1.7181 - sparse_categorical_accuracy: 0.5104\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 11s 111ms/step - loss: 1.6609 - sparse_categorical_accuracy: 0.5236\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 11s 111ms/step - loss: 1.6216 - sparse_categorical_accuracy: 0.5329\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 1.5888 - sparse_categorical_accuracy: 0.5413\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 1.5615 - sparse_categorical_accuracy: 0.5484\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 11s 114ms/step - loss: 1.5451 - sparse_categorical_accuracy: 0.5527\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 11s 114ms/step - loss: 1.5313 - sparse_categorical_accuracy: 0.5564\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 11s 114ms/step - loss: 1.5185 - sparse_categorical_accuracy: 0.5594\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 11s 115ms/step - loss: 1.5022 - sparse_categorical_accuracy: 0.5635\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 11s 109ms/step - loss: 1.4881 - sparse_categorical_accuracy: 0.5669\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 1.4782 - sparse_categorical_accuracy: 0.5690\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 1.4725 - sparse_categorical_accuracy: 0.5702\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 1.4657 - sparse_categorical_accuracy: 0.5714\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 1.4568 - sparse_categorical_accuracy: 0.5734\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 11s 112ms/step - loss: 1.4493 - sparse_categorical_accuracy: 0.5752\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 11s 113ms/step - loss: 1.4357 - sparse_categorical_accuracy: 0.5783\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 1.4369 - sparse_categorical_accuracy: 0.5777\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 12s 119ms/step - loss: 1.4337 - sparse_categorical_accuracy: 0.5781\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 1.4287 - sparse_categorical_accuracy: 0.5791\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 1.4210 - sparse_categorical_accuracy: 0.5809\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 11s 114ms/step - loss: 1.4136 - sparse_categorical_accuracy: 0.5828\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 1.4108 - sparse_categorical_accuracy: 0.5832\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 12s 121ms/step - loss: 1.4097 - sparse_categorical_accuracy: 0.5832\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 1.4067 - sparse_categorical_accuracy: 0.5838\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 1.4025 - sparse_categorical_accuracy: 0.5845\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 12s 117ms/step - loss: 1.3976 - sparse_categorical_accuracy: 0.5860\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 12s 119ms/step - loss: 1.3884 - sparse_categorical_accuracy: 0.5882\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 13s 126ms/step - loss: 1.3927 - sparse_categorical_accuracy: 0.5870\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 12s 124ms/step - loss: 1.3893 - sparse_categorical_accuracy: 0.5878\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 13s 128ms/step - loss: 1.3889 - sparse_categorical_accuracy: 0.5874\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 13s 127ms/step - loss: 1.3833 - sparse_categorical_accuracy: 0.5889\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 12s 122ms/step - loss: 1.3781 - sparse_categorical_accuracy: 0.5904\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 13s 129ms/step - loss: 1.3766 - sparse_categorical_accuracy: 0.5905\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 13s 129ms/step - loss: 1.3825 - sparse_categorical_accuracy: 0.5887\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 13s 129ms/step - loss: 1.3754 - sparse_categorical_accuracy: 0.5908\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 13s 131ms/step - loss: 1.3755 - sparse_categorical_accuracy: 0.5905\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 13s 126ms/step - loss: 1.3701 - sparse_categorical_accuracy: 0.5922\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 13s 126ms/step - loss: 1.3637 - sparse_categorical_accuracy: 0.5936\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 13s 132ms/step - loss: 1.3694 - sparse_categorical_accuracy: 0.5919\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.3685 - sparse_categorical_accuracy: 0.5921\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.3659 - sparse_categorical_accuracy: 0.5926\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 13s 133ms/step - loss: 1.3662 - sparse_categorical_accuracy: 0.5925\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 13s 128ms/step - loss: 1.3594 - sparse_categorical_accuracy: 0.5945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TCBtcpZkykSf"
      },
      "source": [
        "### Make predictions with the model\n",
        "\n",
        "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"o\" and the output probabilities are \"p\" (0.65), \"t\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"Ophelia\" and \"Othello.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tU7M-EGGxR3E",
        "outputId": "c2d31870-0042-43d1-b0f0-ebe17c4279f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " How nent Love,\r\n",
            "  All cur of lordship to a true savend!\r\n",
            "\r\n",
            "14121\r\n",
            "\r\n",
            "Pray, great trutle they are anon!\r\n",
            "\r\n",
            "CLOWN.\r\n",
            "Thats his daughter of blood. Father.\r\n",
            "\r\n",
            "KOFER and Wi'tre of orves; as you ventury? You shall go, liebking calday.\r\n",
            "\r\n",
            "Oood vack of thy su\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " What may,\r\n",
            "But sieted dear liege, and dequired other things,\r\n",
            "Within her Gloucely the thing I shall offendur'd the frame is suck-nor chayd knave, prolond forsweared uttoring my draver to do yonfelble a desery to peer'd to a lover, my virtue is Paris\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " loon;\r\n",
            "You mean's no treasure:\r\n",
            "For it to the sealth reverend I should bring their passion,\r\n",
            "For iteer, lackt to can your morious hell in Venice.\r\n",
            "\r\n",
            "VINCENTIO.\r\n",
            "Good proteent were\r\n",
            "Biding I hear nothing! and sh'rops.\r\n",
            "\r\n",
            "TROILUS.\r\n",
            "My way with as I'll\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " what houating to my pure for serve\r\n",
            "Plond other! You saidors appears_ Vero? Prower ankful malice your soul us'd wire, here not all against my till it us put a scander of her nephony's cwound, thy grave me with Cassio, fairulr'd.\r\n",
            "    I be strong que\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " Even;\r\n",
            "    save the heon, three cannot not lowler hard with English, no, the deaths of their knees the content toward;\r\n",
            "      Yet Troy with them long be gone.\r\n",
            "\r\n",
            "\r\n",
            "      with her friend? all of rest and stands we'll seek, Maye her cast is one, not t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rqE4yy7qlyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}